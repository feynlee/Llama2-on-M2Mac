{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 8819.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: max tensor size =    87.89 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Applications/Anaconda3/envs/py311/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12567eba0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1256800b0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x125681460\n",
      "ggml_metal_init: loaded kernel_scale                          0x125680310\n",
      "ggml_metal_init: loaded kernel_silu                           0x125680570\n",
      "ggml_metal_init: loaded kernel_relu                           0x125682930\n",
      "ggml_metal_init: loaded kernel_gelu                           0x125683010\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1256834b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x125683e90\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x125684500\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x125684d00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x125685570\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x125685bf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1256857d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1256862f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x125686a10\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x125687110\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x125687810\n",
      "ggml_metal_init: loaded kernel_norm                           0x1256886d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x125688f70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x125689710\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x125689f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12568a700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12568b0b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12568b7e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12568bf80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12568c760\n",
      "ggml_metal_init: loaded kernel_rope                           0x12568ceb0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12568d850\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12568e420\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12568edd0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10695d890\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1026.00 MB, ( 8010.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 8412.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   298.00 MB, ( 8710.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, ( 9222.52 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a chatbot\n",
    "# the conversation history will be automatically inserted into the template\n",
    "template = \"\"\"Assistant is a powerful large language model.\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "chatbot = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a powerful large language model.\n",
      "\n",
      "Human: Give me some book recommendations on Quantum Field Theory. List only the book titles and authors.\n",
      "Assistant:\u001b[0m\n",
      " Sure! Here are some excellent books on Quantum Field Theory that I can recommend:\n",
      "\n",
      "* \"Quantum Field Theory for the Gifted Amateur\" by Tom Lancaster and Stephen J. Blundell\n",
      "* \"Quantum Field Theory: A Modern Introduction\" by Daniel M. Green, Michel D. Herren, and John T. Larkoski\n",
      "* \"The Quantum Theory of Fields\" by Steven Weinberg\n",
      "* \"Introduction to Quantum Field Theory\" by Michael Duff and Anthony Zee\n",
      "* \"Quantum Field Theory: A Practical Introduction\" by J.M.C. Cline and S.J. Brodsky\n",
      "\n",
      "Note that the Assistant has only provided book titles and authors, as requested.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 17715.87 ms\n",
      "llama_print_timings:      sample time =   115.26 ms /   158 runs   (    0.73 ms per token,  1370.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17715.83 ms /    39 tokens (  454.25 ms per token,     2.20 tokens per second)\n",
      "llama_print_timings:        eval time = 23793.91 ms /   157 runs   (  151.55 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:       total time = 42119.98 ms\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me some book recommendations on Quantum Field Theory. List only the book titles and authors.\"\n",
    "\n",
    "output = chatbot.predict(human_input=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
